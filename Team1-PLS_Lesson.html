<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Partial Least Squares</title>
    <meta charset="utf-8" />
    <meta name="author" content="Nicholas Emig, Eshan Kaul, Anish Manikonda, Andrew Nutter, Ashwanth Sridhar" />
    <script src="Team1-PLS_Lesson_files/header-attrs-2.20/header-attrs.js"></script>
    <link href="Team1-PLS_Lesson_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
    </script>
    <style>
    .mjx-mrow a {
      color: black;
      pointer-events: none;
      cursor: default;
    }
    </style>
    <link rel="stylesheet" href="assets/tech-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets/tech.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Partial Least Squares
]
.subtitle[
## CMDA 4654 Project 2
]
.author[
### Nicholas Emig, Eshan Kaul, Anish Manikonda, Andrew Nutter, Ashwanth Sridhar
]
.date[
### Group 1
]

---






## Overview
* Background/Motivation

* PLS Algorithm

* Toy Example

* Sports Example

---

## Background

* Partial Least Squares or projection to latent structures is a collection of regression based method designed for the analysis of high dimensional data in a low-structure environment. 

* Developed by econometrician Herman Wold in the late sixties

* Has use in many disciplines including bioinformatics, economics, social science, and **chemometrics**

* Many different variations exist:
.font90.pull-left[
* PLS-W2A
* PLS1
* PLS2
* PLS-SVD
* Mode B PLS (Canonical Correlation Analysis)
* PLS-R
* PLS-DA
* PLS-PM
  ]
.font90.pull-right[
* PLS-SEM
* OPLS
* L-PLS
* SPLS
* Nonlinear Iterative Partial Least Squares (NIPALS)
* SIMPLS
* KernalPLS
]

???

Presenter Notes

PLS-DA = Discriminant Analysis  
PLS-PM = Path Modeling  
PLS-SEM = Squares Path Modeling  
OPLS = Orthogonal Projections to Latent Structures  
SPLS = Sparse Partial Least Squares  

---

## Motivation

* The goal of PLS regression is to predict Y from X and describe their common structure.
* When `\(Y\)` is a vector and `\(X\)` is full rank this can be solved by MLR
* However problems with this arise when `\(X\)` is likely to be singular
  * when the number of predictors/features is large compared to the number of observations 
  * when multicollinearity exists (feature matrix `\(X\)` has less than full rank)  
[For detailed explanation see chapter 3](https://users.cecs.anu.edu.au/~kee/pls.pdf)
---

## What is PLS

* Partial Least Squares (PLS) is a multivariate regression method
* The goal of PLS is to find a low-dimensional representation of the input data that captures the maximum covariance between the input and output variables
* PLS works by iteratively extracting pairs of latent variables or "components" that capture the maximum covariance between the input and output variables
* These components are then used to build a linear regression model
* PLS is a useful method for high-dimensional data with many predictor variables
* PLS can handle multicollinearity and non-linearity in the relationships between the predictor and response variables

---

## Latent Modeling 

![](LatentModel1.svg)
![](LatentModel2.svg)

---

## PCR vs PLS

* PCR uses the `\(X\)`-information to build components
  * Choose basis vectors from low dimensional project that maximize variation of X
  `$$\max\mathrm{Var}(X)$$`
* PLS uses the `\(X\)` and `\(Y\)` information to build components by looking at the cross-covariance of `\(X\)` and `\(Y\)`
  `$$\max\mathrm{Cov}(X,Y)$$`
* PLS and PCR often predict with similar performance &amp; level of error
* PLS generally able to predict with fewer components  

---

## Visulization of PLS
![](geometric-interpretation-of-PLS-step1.png)

---

## Visulization of PLS

![](geometric-interpretation-of-PLS-step2.png)

---

## PLS Algorithm

Let `\(X \in \mathbb{R}^{n \times d}\)` and `\(Y \in \mathbb{R}^{n \times t}\)` be centered matrices, and `\(K\)` is the number of components. Then for each `\(k \in [1, K]\)`:

1. Compute the first pair of singular vectors `\(X_k^T Y_k\)`. `\(u_k \in \mathbb{R}^d\)` and `\(v_k \in \mathbb{R}^t\)`, the first left and right singular vectors of the cross-covariance matrix `\(C = X_k^T Y_k\)`.  

  1A.  Note `\(u_k\)` and `\(v_k\)` are weights that maximize the covariance between the `\(X_k\)` and `\(\text{Cov}(X_k u_k, Y_k v_k)\)`  

2. Project `\(X_k\)` and `\(Y_k\)` on the singular vectors obtaining the scores `\(\xi_k = X_k u_k\)` and `\(\omega_k = Y_k v_k\)`  

3. Obtain the rank-one approximations of the data matrices (loading vectors): 

  3A. Regress `\(X_k\)` on `\(\xi_k\)` to get `\(\gamma_k^T = (\xi_k^T \xi_k)^{-1}\, \xi_k^T X_k\)`  
  
  3B. Regress `\(Y_k\)` on `\(\omega_k\)` to get `\(\delta^T = (\omega_k^T \omega_k)^{-1}\, \omega_k^T Y_k\)`  

---

## PLS Algorithm Continued
  
&lt;span&gt;4.&lt;/span&gt; Deflate `\(X_k\)` and `\(Y_k\)` by subtracting the rank-one approximations:   

&amp;nbsp; &amp;nbsp; 4A. `\(X_{k+1} = X_k - \xi_k \gamma_k^T\)`  
  
&amp;nbsp; &amp;nbsp; 4B. `\(Y_{k + 1} = Y_k - \omega_k \delta_k^T\)`  
  
&lt;span&gt;5.&lt;/span&gt; If `\(X_{k+1}^T Y_{k + 1} = 0\)`  

&amp;nbsp; &amp;nbsp; 5A. `\(K \leftarrow k\)` (Note this is the rank or dimension of the PLS model)  

&amp;nbsp; &amp;nbsp; 5B. Stop/Exit loop  

Else continue.  
  
&lt;span&gt;6.&lt;/span&gt; `\(k \leftarrow k+1\)`  

&lt;span&gt;7.&lt;/span&gt; Return to step 1

---


## PLS Kernel Algorithm 



---

## PLS on mtcars

Step-by-step demonstration on how to perform partial least squares on the built-in R dataset: mtcars

#### Step 1: Load 'pls' Package


```r
# Load pls package
library(pls)

# View first six rows of mtcars dataset
kable(head(mtcars), format = "markdown")
```



|                  |  mpg| cyl| disp|  hp| drat|    wt|  qsec| vs| am| gear| carb|
|:-----------------|----:|---:|----:|---:|----:|-----:|-----:|--:|--:|----:|----:|
|Mazda RX4         | 21.0|   6|  160| 110| 3.90| 2.620| 16.46|  0|  1|    4|    4|
|Mazda RX4 Wag     | 21.0|   6|  160| 110| 3.90| 2.875| 17.02|  0|  1|    4|    4|
|Datsun 710        | 22.8|   4|  108|  93| 3.85| 2.320| 18.61|  1|  1|    4|    1|
|Hornet 4 Drive    | 21.4|   6|  258| 110| 3.08| 3.215| 19.44|  1|  0|    3|    1|
|Hornet Sportabout | 18.7|   8|  360| 175| 3.15| 3.440| 17.02|  0|  0|    3|    2|
|Valiant           | 18.1|   6|  225| 105| 2.76| 3.460| 20.22|  1|  0|    3|    1|
---
## PLS on mtcars
	
#### Step 2: Fit Partial Least Squares model
	
The following code shows how to fit the PLS model to this data.


```r
# Make this example reproducible
set.seed(1)

# Fit PLS model
model &lt;- plsr(hp ~ mpg + disp + drat + wt + qsec, 
              data = mtcars,
              scale = TRUE, 
              validation = "CV")
```
Note:

* scale=TRUE: Ensures that no predictor variable is overly influential in the model if it happens to be measured in different units.

* validation=”CV”: Used for k-fold cross-validation to evaluate the performance of the model. This uses k=10 folds by default.
---
## PLS on mtcars

#### Step 3: Choose the Number of PLS Components


```r
# View summary of model fitting
summary(model)
```

```
Data: 	X dimension: 32 5 
	Y dimension: 32 1
Fit method: kernelpls
Number of components considered: 5

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps
CV           69.66    38.81    34.94    36.08    37.27    37.40
adjCV        69.66    38.69    34.64    35.72    36.82    36.95

TRAINING: % variance explained
    1 comps  2 comps  3 comps  4 comps  5 comps
X     68.66    89.27    95.82    97.94   100.00
hp    71.84    81.74    82.00    82.02    82.03
```
---
## PLS on mtcars


```r
# Visualize cross-validation plot
validationplot(model, val.type = "RMSEP", col = "blue", main = "RMSEP")
```

&lt;img src="Team1-PLS_Lesson_files/figure-html/unnamed-chunk-5-1.png" width="40%" style="display: block; margin: auto;" /&gt;
---
## PLS on mtcars

#### Step 4: Make Predictions using Final Model


```r
# Define training and testing sets
train &lt;- mtcars[1:25, c("hp", "mpg", "disp", "drat", "wt", "qsec")]
y_test &lt;- mtcars[26:nrow(mtcars), c("hp")]
test &lt;- mtcars[26:nrow(mtcars), c("mpg", "disp", "drat", "wt", "qsec")]
    
# Use model to make predictions on a test set
model_pls &lt;- plsr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")
model_pcr &lt;- pcr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")

pls_pred &lt;- predict(model_pls, test, ncomp=2)
pcr_pred &lt;- predict(model_pcr, test, ncomp=2)

# Calculate RMSE
rmse_pls &lt;- sqrt(mean((pls_pred - y_test)^2))
rmse_pcr &lt;- sqrt(mean((pcr_pred - y_test)^2))
```
---
## PLS on mtcars

#### Results: 

```
 PLS test RMSE: 54.9 
 PCR test RMSE: 56.9 
```

* A lower RMSE value indicates better prediction performance. 
* Therefore, based on this specific dataset and model setup, the PLS model outperformed the PCR model with respect to predicting hp.

---

## References
.font60[
* Abdi H, Williams LJ. Partial least squares methods: partial least squares correlation and partial least square regression. Methods Mol Biol. 2013;930:549-79. doi: 10.1007/978-1-62703-059-5_23. PMID: 23086857.

* Dunn, Kevin. “6.7 A Conceptual Explanation of PLS.” Process Improvement Using Data, 1 Feb. 2023, https://learnche.org/pid/latent-variable-modelling/projection-to-latent-structures/conceptual-mathematical-and-geometric-interpretation-of-pls. 

* Kristian Hovde Liland [aut, cre]. (2022, July 16). PLS: Partial least squares and principal component regression version 2.8-1 from cran. version 2.8-1 from CRAN. Retrieved April 24, 2023, from https://rdrr.io/cran/pls/ 

* Rosipal, R., Krämer, N. (2006). Overview and Recent Advances in Partial Least Squares. In: Saunders, C., Grobelnik, M., Gunn, S., Shawe-Taylor, J. (eds) Subspace, Latent Structure and Feature Selection. SLSFS 2005. Lecture Notes in Computer Science, vol 3940. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11752790_2

* Wegelin, J.A. (2000). A Survey of Partial Least Squares (PLS) Methods, with Emphasis on the Two-Block Case.

* 1.8. cross decomposition. scikit. (2023). Retrieved April 24, 2023, from https://scikit-learn.org/stable/modules/cross_decomposition.html#cross-decomposition 
]

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
