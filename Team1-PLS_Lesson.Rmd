---
title: "Partial Least Squares"
subtitle: "CMDA 4654 Project 2"
author: "Nicholas Emig, Eshan Kaul, Anish Manikonda, Andrew Nutter, Ashwanth Sridhar"
date: "Group 1"
output:
  xaringan::moon_reader:
    css: ["default", "assets/tech-fonts.css", "assets/tech.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
library(knitr)
```

```{r, load_refs, echo=FALSE, cache=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
myBib <- ReadBib("assets/example.bib", check = FALSE)
top_icon = function(x) {
  icons::icon_style(
    icons::fontawesome(x),
    position = "fixed", top = 10, right = 10
  )
}
```

## Overview
* Background/Motivation

* PLS Algorithm

* Toy Example

* Sports Example

---

## Background

* Partial Least Squares or projection to latent structures is a collection of regression based method designed for the analysis of high dimensional data in a low-structure environment. 

* Developed by econometrician Herman Wold in the late sixties

* Has use in many disciplines including bioinformatics, economics, social science, and **chemometrics**

* Many different variations exist:
.font90.pull-left[
* PLS-W2A
* PLS1
* PLS2
* PLS-SVD
* Mode B PLS (Canonical Correlation Analysis)
* PLS-R
* PLS-DA
* PLS-PM
  ]
.font90.pull-right[
* PLS-SEM
* OPLS
* L-PLS
* SPLS
* Nonlinear Iterative Partial Least Squares (NIPALS)
* SIMPLS
* KernalPLS
]

???

Presenter Notes

PLS-DA = Discriminant Analysis  
PLS-PM = Path Modeling  
PLS-SEM = Squares Path Modeling  
OPLS = Orthogonal Projections to Latent Structures  
SPLS = Sparse Partial Least Squares  

---

## Motivation

* The goal of PLS regression is to predict Y from X and describe their common structure.
* When $Y$ is a vector and $X$ is full rank this can be solved by MLR
* However problems with this arise when $X$ is likely to be singular
  * when the number of predictors/features is large compared to the number of observations 
  * when multicollinearity exists (feature matrix $X$ has less than full rank)  
[For detailed explanation see chapter 3](https://users.cecs.anu.edu.au/~kee/pls.pdf)
---

## What is PLS

* Partial Least Squares (PLS) is a multivariate regression method
* The goal of PLS is to find a low-dimensional representation of the input data that captures the maximum covariance between the input and output variables
* PLS works by iteratively extracting pairs of latent variables or "components" that capture the maximum covariance between the input and output variables
* These components are then used to build a linear regression model
* PLS is a useful method for high-dimensional data with many predictor variables
* PLS can handle multicollinearity and non-linearity in the relationships between the predictor and response variables

---

## Latent Modeling 

![](LatentModel1.svg)
![](LatentModel2.svg)

---

## PCR vs PLS

* PCR uses the $X$-information to build components
  * Choose basis vectors from low dimensional project that maximize variation of X
  $$\max\mathrm{Var}(X)$$
* PLS uses the $X$ and $Y$ information to build components by looking at the cross-covariance of $X$ and $Y$
  $$\max\mathrm{Cov}(X,Y)$$
* PLS and PCR often predict with similar performance & level of error
* PLS generally able to predict with fewer components  

---

## Visulization of PLS
![](geometric-interpretation-of-PLS-step1.png)

---

## Visulization of PLS

![](geometric-interpretation-of-PLS-step2.png)

---

## PLS Algorithm

Let $X \in \mathbb{R}^{n \times d}$ and $Y \in \mathbb{R}^{n \times t}$ be centered matrices, and $K$ is the number of components. Then for each $k \in [1, K]$:

1. Compute the first pair of singular vectors $X_k^T Y_k$. $u_k \in \mathbb{R}^d$ and $v_k \in \mathbb{R}^t$, the first left and right singular vectors of the cross-covariance matrix $C = X_k^T Y_k$.  

  1A.  Note $u_k$ and $v_k$ are weights that maximize the covariance between the $X_k$ and $\text{Cov}(X_k u_k, Y_k v_k)$  

2. Project $X_k$ and $Y_k$ on the singular vectors obtaining the scores $\xi_k = X_k u_k$ and $\omega_k = Y_k v_k$  

3. Obtain the rank-one approximations of the data matrices (loading vectors): 

  3A. Regress $X_k$ on $\xi_k$ to get $\gamma_k^T = (\xi_k^T \xi_k)^{-1}\, \xi_k^T X_k$  
  
  3B. Regress $Y_k$ on $\omega_k$ to get $\delta^T = (\omega_k^T \omega_k)^{-1}\, \omega_k^T Y_k$  

---

## PLS Algorithm Continued
  
<span>4.</span> Deflate $X_k$ and $Y_k$ by subtracting the rank-one approximations:   

&nbsp; &nbsp; 4A. $X_{k+1} = X_k - \xi_k \gamma_k^T$  
  
&nbsp; &nbsp; 4B. $Y_{k + 1} = Y_k - \omega_k \delta_k^T$  
  
<span>5.</span> If $X_{k+1}^T Y_{k + 1} = 0$  

&nbsp; &nbsp; 5A. $K \leftarrow k$ (Note this is the rank or dimension of the PLS model)  

&nbsp; &nbsp; 5B. Stop/Exit loop  

Else continue.  
  
<span>6.</span> $k \leftarrow k+1$  

<span>7.</span> Return to step 1

---


## PLS Kernel Algorithm 

```{r}

```

---

## PLS on mtcars

Step-by-step demonstration on how to perform partial least squares on the built-in R dataset: mtcars

#### Step 1: Load 'pls' Package

```{r, message = FALSE}
# Load pls package
library(pls)

# View first six rows of mtcars dataset
kable(head(mtcars), format = "markdown")

```
---
## PLS on mtcars
	
#### Step 2: Fit Partial Least Squares model
	
The following code shows how to fit the PLS model to this data.

```{r}
# Make this example reproducible
set.seed(1)

# Fit PLS model
model <- plsr(hp ~ mpg + disp + drat + wt + qsec, 
              data = mtcars,
              scale = TRUE, 
              validation = "CV")
```
Note:

* scale=TRUE: Ensures that no predictor variable is overly influential in the model if it happens to be measured in different units.

* validation=”CV”: Used for k-fold cross-validation to evaluate the performance of the model. This uses k=10 folds by default.
---
## PLS on mtcars

#### Step 3: Choose the Number of PLS Components

```{r}
# View summary of model fitting
summary(model)
```
---
## PLS on mtcars

```{r, fig.align='center', out.width = "40%"}
# Visualize cross-validation plot
validationplot(model, val.type = "RMSEP", col = "blue", main = "RMSEP")
```
---
## PLS on mtcars

#### Step 4: Make Predictions using Final Model

```{r, results='hide'}
# Define training and testing sets
train <- mtcars[1:25, c("hp", "mpg", "disp", "drat", "wt", "qsec")]
y_test <- mtcars[26:nrow(mtcars), c("hp")]
test <- mtcars[26:nrow(mtcars), c("mpg", "disp", "drat", "wt", "qsec")]
    
# Use model to make predictions on a test set
model_pls <- plsr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")
model_pcr <- pcr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")

pls_pred <- predict(model_pls, test, ncomp=2)
pcr_pred <- predict(model_pcr, test, ncomp=2)

# Calculate RMSE
rmse_pls <- sqrt(mean((pls_pred - y_test)^2))
rmse_pcr <- sqrt(mean((pcr_pred - y_test)^2))
```
---
## PLS on mtcars

#### Results: 
```{r, echo=FALSE}
cat(" PLS test RMSE:", round(rmse_pls, 1),"\n",
   "PCR test RMSE:", round(rmse_pcr, 1), "\n")
```

* A lower RMSE value indicates better prediction performance. 
* Therefore, based on this specific dataset and model setup, the PLS model outperformed the PCR model with respect to predicting hp.

---

## References
.font60[
* Abdi H, Williams LJ. Partial least squares methods: partial least squares correlation and partial least square regression. Methods Mol Biol. 2013;930:549-79. doi: 10.1007/978-1-62703-059-5_23. PMID: 23086857.

* Dunn, Kevin. “6.7 A Conceptual Explanation of PLS.” Process Improvement Using Data, 1 Feb. 2023, https://learnche.org/pid/latent-variable-modelling/projection-to-latent-structures/conceptual-mathematical-and-geometric-interpretation-of-pls. 

* Kristian Hovde Liland [aut, cre]. (2022, July 16). PLS: Partial least squares and principal component regression version 2.8-1 from cran. version 2.8-1 from CRAN. Retrieved April 24, 2023, from https://rdrr.io/cran/pls/ 

* Rosipal, R., Krämer, N. (2006). Overview and Recent Advances in Partial Least Squares. In: Saunders, C., Grobelnik, M., Gunn, S., Shawe-Taylor, J. (eds) Subspace, Latent Structure and Feature Selection. SLSFS 2005. Lecture Notes in Computer Science, vol 3940. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11752790_2

* Wegelin, J.A. (2000). A Survey of Partial Least Squares (PLS) Methods, with Emphasis on the Two-Block Case.

* 1.8. cross decomposition. scikit. (2023). Retrieved April 24, 2023, from https://scikit-learn.org/stable/modules/cross_decomposition.html#cross-decomposition 
]

